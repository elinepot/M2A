{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the properties of variational approximations of Gibbs posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import torch.distributions as dist\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.pi = torch.acos(torch.zeros(1))*2 # 3.1415927410125732\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss 0-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss01(nn.Module):\n",
    "    def __init__(self, p, v, lambda_val, B, n, m = None, si=None):\n",
    "        super(Loss01, self).__init__()\n",
    "        self.v = v\n",
    "        self.lambda_val = lambda_val\n",
    "        self.B = B\n",
    "        self.m = nn.Parameter(m) if m is not None else nn.Parameter(torch.rand(p))\n",
    "        self.si = nn.Parameter(si) if si is not None else nn.Parameter(torch.rand(1))\n",
    "        torch.manual_seed(0)\n",
    "        self.normal_dist = dist.Normal(torch.zeros(n), torch.ones(n))\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        n, p = X.shape\n",
    "        # Get random indices\n",
    "        torch.manual_seed(0)\n",
    "        indices = torch.randint(0, n, (self.B,)) if n != self.B else torch.arange(self.B)\n",
    "        X_selected = X[indices]\n",
    "        Y_selected = Y[indices]\n",
    "        \n",
    "        G = Y_selected * X_selected\n",
    "        nG = torch.linalg.norm(G, dim=1)\n",
    "        mG = torch.sum(G * self.m, dim=1)\n",
    "        denom = torch.sqrt(torch.linalg.norm(X_selected, dim=1)) * self.si \n",
    "        term = mG / denom\n",
    "        \n",
    "        loss = self.normal_dist.cdf(-term)\n",
    "        reg_term = 0.5 * torch.dot(self.m.flatten(), self.m.flatten()) / self.v**2\n",
    "        \n",
    "        loss = (self.lambda_val / self.B) * loss + reg_term - 0.5 * p * (torch.log(self.si ** 2) - self.si ** 2 / self.v**2)\n",
    "        return loss.mean()\n",
    "    \n",
    "    def bound(self, X, Y, c_x, eps=0.01):\n",
    "        n, p = X.shape\n",
    "        eps = torch.tensor(eps)\n",
    "        f = self.lambda_val/(2*n)\n",
    "        KL = 0.5*(p*self.si**2/self.v**2 - p + torch.dot(self.m.flatten(), self.m.flatten()) / self.v**2 + p * torch.log(self.v**2 / self.si**2))\n",
    "        return (f + KL + torch.log(1/eps))/self.lambda_val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hinge(nn.Module):\n",
    "    def __init__(self, p, v, lambda_, B, n, m = None, si = None):\n",
    "        super(Hinge, self).__init__()\n",
    "        self.v = v\n",
    "        self.lambda_ = lambda_\n",
    "        self.B = B\n",
    "        self.m = nn.Parameter(m) if m is not None else nn.Parameter(torch.rand(p))\n",
    "        self.si = nn.Parameter(si) if si is not None else nn.Parameter(torch.rand(1))\n",
    "        torch.manual_seed(0)\n",
    "        self.normal_dist = dist.Normal(torch.zeros(n), torch.ones(n))\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        n, p = X.shape\n",
    "        # Get random indices\n",
    "        indices = torch.randint(0, n, (self.B,)) if n != self.B else torch.arange(self.B)\n",
    "        X_selected = X[indices]\n",
    "        Y_selected = Y[indices]\n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        G = Y_selected * X_selected\n",
    "        G = G.view(self.B, p)\n",
    "        nG = torch.linalg.norm(G, dim=1)  # |Y_iX_i|\n",
    "        mG = torch.sum(G * self.m, dim=1) # <Y_iX_i, m>\n",
    "        ratio = (1 - mG) / (self.si * nG) # (1 - <Y_iX_i, m>) / (si * |Y_iX_i|)\n",
    "        term1 = (1 - mG) *  self.normal_dist.cdf(ratio) # (1 - <Y_iX_i, m>) * Phi(ratio)\n",
    "        term2 = self.si * nG * torch.exp(-0.5 * ratio ** 2) / torch.sqrt(2 * torch.tensor(np.pi)) # si * |Y_iX_i| * phi(ratio)\n",
    "        sum_val = torch.sum(term1 + term2)\n",
    "        \n",
    "        loss = (self.lambda_ / self.B) * sum_val + 0.5 * torch.dot(self.m.view(-1), self.m.view(-1)) / self.v**2 - 0.5 * p * (torch.log(self.si ** 2) - self.si ** 2 / self.v**2)\n",
    "        return loss\n",
    "    \n",
    "    def bound(self, X, Y, c_x = None, eps=0.01):\n",
    "        n, p = X.shape\n",
    "        c_x = torch.tensor(torch.max(X) + 0.1) if c_x is None else torch.tensor(c_x)\n",
    "        eps = torch.tensor(eps)\n",
    "        if (1-self.v**2 * self.lambda_**2 * c_x**2/2*n).item() > 0:   \n",
    "            f = self.lambda_**2/(4*n) - 0.5*torch.log(1-self.v**2 * self.lambda_**2 * c_x**2/2*n)\n",
    "        else:\n",
    "            return torch.tensor(-1)\n",
    "        KL = 0.5*(p*self.si**2/self.v**2 - p + torch.dot(self.m.flatten(), self.m.flatten()) / self.v**2 + p * torch.log(self.v**2 / self.si**2))\n",
    "        return (f + KL + torch.log(1/eps))/self.lambda_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exponential(nn.Module):\n",
    "    def __init__(self, p, v, lambda_, B, n, m= None, si = None):\n",
    "        super(Exponential, self).__init__()\n",
    "        self.v = v\n",
    "        self.lambda_ = lambda_\n",
    "        self.B = B\n",
    "        self.m = nn.Parameter(m) if m is not None else nn.Parameter(torch.rand(p))\n",
    "        self.si = nn.Parameter(si) if si is not None else nn.Parameter(torch.rand(1))\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        n, p = X.shape\n",
    "        # Get random indices\n",
    "        indices = torch.randint(0, n, (self.B,)) if n != self.B else torch.arange(self.B)\n",
    "        X_selected = X[indices]\n",
    "        Y_selected = Y[indices]\n",
    "        \n",
    "        G = Y_selected * X_selected\n",
    "        G = G.view(self.B, p)\n",
    "        nG = torch.linalg.norm(G, dim=1) #|Yi Xi|\n",
    "        mG = torch.sum(G * self.m, dim=1) #|Yi Xi| * m\t\n",
    "        term = torch.exp(-mG + 0.5 * (self.si ** 2) * (nG ))\n",
    "        sum_val = torch.sum(term)\n",
    "        \n",
    "        loss = (self.lambda_ / self.B) * sum_val + 0.5 * torch.dot(self.m.view(-1), self.m.view(-1)) / self.v**2 - 0.5 * p * (torch.log(self.si ** 2) - self.si ** 2 / self.v**2)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator(X, theta):\n",
    "    return np.sign(np.dot(X, theta.T))\n",
    "\n",
    "# misclassification rate\n",
    "def misclassification_rate(X, Y, m, si):\n",
    "    n = X.shape[0]\n",
    "    X = X.detach().numpy()\n",
    "    Y = Y.detach().numpy()\n",
    "    \n",
    "    # on va générer des theta, calculer les estimateurs associés, puis la moyenne des estimateurs:\n",
    "    np.random.seed(0)\n",
    "    theta = np.random.normal(m, si, (1000, len(m)))\n",
    "    estimators = np.array([estimator(X, theta_) for theta_ in theta])\n",
    "    mean_estimates = np.sign(np.mean(estimators, axis=0)).reshape((-1,1))\n",
    "    error = np.mean(mean_estimates != Y)\n",
    "    return error\n",
    "\n",
    "# hinge loss\n",
    "def hinge_misclassification_rate(X, Y, m, si):\n",
    "    n = X.shape[0]\n",
    "    X = X.detach().numpy()\n",
    "    Y = Y.detach().numpy()\n",
    "    np.random.seed(0)\n",
    "    theta = np.random.normal(m, si, (1000, len(m)))\n",
    "    estimators = np.array([estimator(X, theta_) for theta_ in theta])\n",
    "    mean_estimates = np.sign(np.mean(estimators, axis=0)).reshape((-1,1))\n",
    "    error = np.mean(np.maximum(0, 1 - mean_estimates * Y))\n",
    "    return error\n",
    "\n",
    "# exponential loss\n",
    "def exponential_loss(Y_hat, Y):\n",
    "    return np.exp(-Y_hat * Y)\n",
    "\n",
    "def exp_loss_misclassification_rate(X, Y, m, si):\n",
    "    n = X.shape[0]\n",
    "    X = X.detach().numpy()\n",
    "    Y = Y.detach().numpy()\n",
    "    np.random.seed(0)\n",
    "    theta = np.random.normal(m, si, (1000, len(m)))\n",
    "    estimators = np.array([estimator(X, theta_) for theta_ in theta])\n",
    "    mean_estimates = np.sign(np.mean(estimators, axis=0)).reshape((-1,1))\n",
    "    error = np.mean(exponential_loss(mean_estimates,Y))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison with logisitc regression and SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compare VB with Logistic Regressor & SVM :\n",
    "def compare_logisitic(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    # change the tensors into np: \n",
    "    X_train, Y_train, X_test, Y_test = map(torch.Tensor.numpy, (Xtrain, Ytrain, Xtest, Ytest))\n",
    "    print('Logistic regression : ')\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    y_pred_clf = clf.predict(X_test)\n",
    "    error = np.mean(y_pred_clf != Y_test)\n",
    "    print('error logistic regression : ', error)\n",
    "    print('error hinge loss : ', np.maximum(0, 1 - y_pred_clf * Y_test).mean())\n",
    "    print('exponential loss : ', np.exp(-y_pred_clf * Y_test).mean())\n",
    "\n",
    "def compare_SVM(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    # change the tensors into np: \n",
    "    X_train, Y_train, X_test, Y_test = map(torch.Tensor.numpy, (Xtrain, Ytrain, Xtest, Ytest))\n",
    "    print('\\nSVM : ')\n",
    "    svm = SVC(kernel='sigmoid', random_state=0)\n",
    "    svm.fit(X_train, Y_train)\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    error = np.mean(y_pred_svm != Y_test)\n",
    "    print('error SVM : ', error)\n",
    "    print('error hinge loss : ', np.maximum(0, 1 - y_pred_svm * Y_test).mean())\n",
    "    print('exponential loss : ', np.exp(-y_pred_svm * Y_test).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimization process for any loss function type (loss_type in {'01', 'hinge', 'exp'})\n",
    "model_classes = {\n",
    "    'hinge': Hinge,\n",
    "    'exp': Exponential,\n",
    "    '01': Loss01\n",
    "}\n",
    "\n",
    "def training(Xtrain, Ytrain, Xtest, Ytest, loss_type = 'hinge', n_epochs = 1000, lr=0.01, m = None, si = None, divide_by_c_x = True, verbose = True):\n",
    "    n, p = Xtrain.shape\n",
    "    c_x = torch.max(Xtrain) + 0.01\n",
    "    v = 1/np.sqrt(p)\n",
    "    lambda_ = np.sqrt(n*p)\n",
    "    if divide_by_c_x:\n",
    "        lambda_ /= c_x\n",
    "    B = n\n",
    "\n",
    "    model_class = model_classes.get(loss_type)\n",
    "    if model_class is not None:\n",
    "        model = model_class(p, v, lambda_, B, n, m, si)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown loss_type: {}\".format(loss_type))\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"\\n Training with model {loss_type} ...\")\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(Xtrain, Ytrain)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if verbose and epoch % 100 == 0:\n",
    "            print(f\"  Epoch {epoch}, loss: {loss.item()}\")\n",
    "\n",
    "    final_m = np.copy(model.m.detach().numpy())\n",
    "    final_si = np.copy(model.si.detach().numpy())\n",
    "\n",
    "    print(\"\\n  misclassification rate : \", misclassification_rate(Xtest, Ytest, final_m, final_si))\n",
    "\n",
    "    if loss_type == 'hinge':\n",
    "        print(\"  hinge misclassification rate : \", hinge_misclassification_rate(Xtest, Ytest, final_m, final_si))\n",
    "    elif loss_type == 'exp':\n",
    "        print(\"  exp misclassification rate : \", exp_loss_misclassification_rate(Xtest, Ytest, final_m, final_si))\n",
    "\n",
    "    if loss_type != 'exp':\n",
    "        bound = model.bound(Xtest, Ytest, c_x)\n",
    "        print('  bound : ', bound.item() if type(bound) != int else bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training with model 01 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 212.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  misclassification rate :  0.18421052631578946\n",
      "  bound :  0.04322889819741249\n",
      "\n",
      " Training with model hinge ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 221.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  misclassification rate :  0.017543859649122806\n",
      "  hinge misclassification rate :  0.03508771929824561\n",
      "  bound :  -1\n",
      "\n",
      " Training with model exp ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:18<00:00, 533.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  misclassification rate :  0.08771929824561403\n",
      "  exp misclassification rate :  0.5740550891791268\n",
      "\n",
      "Comparaison with logistice regression and SVM : \n",
      "Logistic regression : \n",
      "error logistic regression :  0.3616497383810403\n",
      "error hinge loss :  0.7232995\n",
      "exponential loss :  1.2179021\n",
      "\n",
      "SVM : \n",
      "error SVM :  0.35687903970452445\n",
      "error hinge loss :  0.71375805\n",
      "exponential loss :  1.2066889\n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "y = y.replace({'M': 1, 'B': -1})\n",
    "Y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "# normalizing the data\n",
    "X = X.values\n",
    "m = np.mean(X, axis=0)\n",
    "v = np.std(X, axis=0)\n",
    "X = (X - m) / v\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "N = X.shape[0]\n",
    "n = int(0.8 * N)\n",
    "\n",
    "Xtrain, Xtest = X[:n], X[n:]\n",
    "Ytrain, Ytest = Y[:n], Y[n:]\n",
    "p = Xtrain.shape[1]\n",
    "\n",
    "training(Xtrain, Ytrain, Xtest, Ytest, loss_type = '01', n_epochs = 1000, lr=0.01, m = None, si = None, divide_by_c_x=False, verbose = False)\n",
    "training(Xtrain, Ytrain, Xtest, Ytest, loss_type = 'hinge', n_epochs = 1000, lr=0.01, m = None, si = None, divide_by_c_x=False,  verbose = False)\n",
    "training(Xtrain, Ytrain, Xtest, Ytest, loss_type = 'exp', n_epochs = 10_000, lr=0.01, m = None, si = None, divide_by_c_x=False,verbose = False)\n",
    "\n",
    "print(\"\\nComparaison with logistice regression and SVM : \")\n",
    "compare_logisitic(Xtrain, Ytrain, Xtest, Ytest)\n",
    "compare_SVM(Xtrain, Ytrain, Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Scept Heart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training with model 01 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 253.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  misclassification rate :  0.3888888888888889\n",
      "  bound :  0.0750739648938179\n",
      "\n",
      " Training with model hinge ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 219.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  misclassification rate :  0.14814814814814814\n",
      "  hinge misclassification rate :  0.2962962962962963\n",
      "  bound :  -1\n",
      "\n",
      " Training with model exp ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:18<00:00, 550.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  misclassification rate :  0.12962962962962962\n",
      "  exp misclassification rate :  0.6725612321161316\n",
      "\n",
      "Comparaison with logistice regression and SVM : \n",
      "Logistic regression : \n",
      "error logistic regression :  0.3683127572016461\n",
      "error hinge loss :  0.7366255\n",
      "exponential loss :  1.2335627\n",
      "\n",
      "SVM : \n",
      "error SVM :  0.2777777777777778\n",
      "error hinge loss :  0.5555556\n",
      "exponential loss :  1.0207692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spect_heart = fetch_ucirepo(id=95) \n",
    " \n",
    "X = spect_heart.data.features \n",
    "y = spect_heart.data.targets \n",
    "y = y.replace({0: -1})\n",
    "Y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "# normalizing the data\n",
    "X = X.values\n",
    "m = np.mean(X, axis=0)\n",
    "v = np.std(X, axis=0)\n",
    "X = (X - m) / v\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "N = X.shape[0]\n",
    "n = int(0.8 * N)\n",
    "\n",
    "Xtrain, Xtest = X[:n], X[n:]\n",
    "Ytrain, Ytest = Y[:n], Y[n:]\n",
    "p = Xtrain.shape[1]\n",
    "\n",
    "training(Xtrain, Ytrain, Xtest, Ytest, loss_type = '01', n_epochs = 1000, lr=0.01, m = None, si = None, divide_by_c_x=False, verbose = False)\n",
    "training(Xtrain, Ytrain, Xtest, Ytest, loss_type = 'hinge', n_epochs = 1000, lr=0.01, m = None, si = None, divide_by_c_x=False, verbose = False)\n",
    "training(Xtrain, Ytrain, Xtest, Ytest, loss_type = 'exp', n_epochs = 10_000, lr=0.01, m = None, si = None, divide_by_c_x=False, verbose = False)\n",
    "\n",
    "print(\"\\nComparaison with logistice regression and SVM : \")\n",
    "compare_logisitic(Xtrain, Ytrain, Xtest, Ytest)\n",
    "compare_SVM(Xtrain, Ytrain, Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Students Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training with model 01 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:32<00:00, 154.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  misclassification rate :  0.3446327683615819\n",
      "  bound :  0.013292035087943077\n",
      "\n",
      " Training with model hinge ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 132.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  misclassification rate :  0.15254237288135594\n",
      "  hinge misclassification rate :  0.3050847457627119\n",
      "  bound :  -1\n",
      "\n",
      " Training with model exp ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:38<00:00, 258.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  misclassification rate :  0.327683615819209\n",
      "  exp misclassification rate :  1.1380677940679451\n",
      "\n",
      "Comparaison with logistice regression and SVM : \n",
      "Logistic regression : \n",
      "error logistic regression :  0.43170864055667274\n",
      "error hinge loss :  0.86341727\n",
      "exponential loss :  1.3825687\n",
      "\n",
      "SVM : \n",
      "error SVM :  0.4415397874174088\n",
      "error hinge loss :  0.8830796\n",
      "exponential loss :  1.4056758\n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "predict_students_dropout_and_academic_success = fetch_ucirepo(id=697) \n",
    " \n",
    "X = predict_students_dropout_and_academic_success.data.features \n",
    "y = predict_students_dropout_and_academic_success.data.targets \n",
    "y.replace({'Graduate': 1, 'Dropout': -1, 'Enrolled':1}, inplace=True)\n",
    "Y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "# normalizing the data\n",
    "X = X.values\n",
    "m = np.mean(X, axis=0)\n",
    "v = np.std(X, axis=0)\n",
    "X = (X - m) / v\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "N = X.shape[0]\n",
    "n = int(0.8 * N)\n",
    "\n",
    "Xtrain, Xtest = X[:n], X[n:]\n",
    "Ytrain, Ytest = Y[:n], Y[n:]\n",
    "p = Xtrain.shape[1]\n",
    "\n",
    "training(Xtrain, Ytrain, Xtest, Ytest, loss_type = '01', n_epochs = 5000, lr=0.005, m = None, si = None, divide_by_c_x=False, verbose = False)\n",
    "training(Xtrain, Ytrain, Xtest, Ytest, loss_type = 'hinge', n_epochs = 1000, lr=0.01, m = None, si = None, divide_by_c_x=False, verbose = False)\n",
    "training(Xtrain, Ytrain, Xtest, Ytest, loss_type = 'exp', n_epochs = 10_000, lr=0.01, m = None, si = None, divide_by_c_x=False, verbose = False)\n",
    "\n",
    "print(\"\\nComparaison with logistice regression and SVM : \")\n",
    "compare_logisitic(Xtrain, Ytrain, Xtest, Ytest)\n",
    "compare_SVM(Xtrain, Ytrain, Xtest, Ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
